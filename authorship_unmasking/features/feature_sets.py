# Copyright (C) 2017-2019 Janek Bevendorff, Webis Group
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from conf.interfaces import instance_property, instance_list_property
from features.interfaces import ChunkSampler, FeatureSet
from input.interfaces import SamplePair, Tokenizer
from input.tokenizers import WordTokenizer, CharNgramTokenizer, DisjunctCharNgramTokenizer
from util.util import lru_cache

from copy import deepcopy
import numpy
from math import ceil
from nltk import FreqDist

from typing import List, Iterable


class MetaFeatureSet(FeatureSet):
    """
    Feature set combining features of several feature sets into a single feature vector
    of the given length. Vector lengths of sub feature are according to the configured
    sub feature proportions.
    """
    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None):
        super().__init__(pair, sampler)

        self._sub_features = []                 # type: List[FeatureSet]
        self._sub_feature_proportions = []      # type: List[int]

    def add_sub_feature(self, feature: FeatureSet):
        """
        Add an individual sub feature.

        :param feature: feature set to add
        """
        self._sub_features.append(feature)

    @instance_list_property(delegate_args=True)
    def sub_features(self) -> List[FeatureSet]:
        """ Get sub features. """
        return self._sub_features

    @sub_features.setter
    def sub_features(self, features: List[FeatureSet]):
        """ Set sub features. """
        self._sub_features = features

    @property
    def feature_proportions(self) -> List[int]:
        """ Get sub feature proportions. """
        return self._sub_feature_proportions

    @feature_proportions.setter
    def feature_proportions(self, proportions: List[int]):
        """ Set sub feature proportions. """
        self._sub_feature_proportions = proportions

    def _get_features(self, n, func) -> Iterable[numpy.ndarray]:
        sum_weights = sum(self._sub_feature_proportions)
        proportions = numpy.concatenate((self._sub_feature_proportions,
                                        numpy.ones(len(self._sub_features) - len(self._sub_feature_proportions))))
        proportions = [ceil(n * w / sum_weights) for w in proportions]

        return numpy.concatenate([list(getattr(f, func)(proportions[i]))
                                  for i, f in enumerate(self._sub_features)], axis=1)

    def get_features_absolute(self, n: int) -> Iterable[numpy.ndarray]:
        return self._get_features(n, "get_features_absolute")

    def get_features_relative(self, n: int) -> Iterable[numpy.ndarray]:
        return self._get_features(n, "get_features_relative")


class MultiChunkFeatureSet(MetaFeatureSet):
    """
    Meta feature set to be used with chunks generated by a :class::MultiChunker.
    The number of sub features of this FeatureSet has to be the same as the number
    of sub chunks of the MultiChunker.
    """
    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None):
        super().__init__(pair, sampler)

        self._sub_features_initialized = False

    def _get_features(self, n, func) -> Iterable[numpy.ndarray]:
        if not self._sub_features_initialized:
            for i, feature in enumerate(self._sub_features):
                if len(self._sub_features) != len(self.pair.chunks_a[i]) != len(self.pair.chunks_b[i]):
                    raise ValueError("Number of sub chunks needs to be the same as number of sub features.")

                new_pair = deepcopy(self.pair)
                new_pair.pair_id = self.pair.pair_id
                new_chunks_a = [c[i] for c in new_pair.chunks_a]
                new_chunks_b = [c[i] for c in new_pair.chunks_b]
                new_pair.replace_chunks(new_chunks_a, new_chunks_b)
                feature.pair = new_pair
            self._sub_features_initialized = True

        return super()._get_features(n, func)


class CachedAvgTokenCountFeatureSet(FeatureSet):
    """
    Generic feature set which uses the average frequency counts per chunk of the
    tokens generated by a specified tokenizer and caches them in memory.
    By default, the cache size is limited to 2000 chunks.
    """
    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None, chunk_tokenizer: Tokenizer = None):
        """
        :param pair: pair of chunked texts
        :param sampler: :class:`ChunkSampler` for sampling chunks from ``pair``
        :param chunk_tokenizer: tokenizer for tokenizing chunks
        """
        super().__init__(pair, sampler)

        self._chunk_tokenizer = chunk_tokenizer
        self._is_prepared = False

        self.__freq_a = None
        self.__freq_b = None
        self._chunks  = []

    @instance_property
    def chunk_tokenizer(self) -> Tokenizer:
        return self._chunk_tokenizer

    @chunk_tokenizer.setter
    def chunk_tokenizer(self, tokenizer):
        self._chunk_tokenizer = tokenizer

    @FeatureSet.pair.setter
    def pair(self, pair):
        self._pair = pair
        self._chunks = []
        self._is_prepared = False

    def _prepare(self):
        if self._is_prepared:
            return

        freq_dist_a = FreqDist()
        for a in self._pair.chunks_a:
            freq_dist_a.update(self._tokenize(a))

        freq_dist_b = FreqDist()
        for b in self._pair.chunks_b:
            freq_dist_b.update(self._tokenize(b))

        self._avg_freq_dist = FreqDist()
        n_a = freq_dist_a.N()
        n_b = freq_dist_b.N()
        for a in freq_dist_a:
            self._avg_freq_dist[a] = (freq_dist_a[a] / n_a + freq_dist_b[a] / n_b) / 2.0
        for b in freq_dist_b:
            if self._avg_freq_dist[b] != 0.0:
                continue
            self._avg_freq_dist[b] = (freq_dist_a[b] / n_a + freq_dist_b[b] / n_b) / 2.0

        self._chunks = self._sampler.generate_chunk_pairs(self._pair)

        self.__freq_a = None
        self.__freq_b = None

        self._is_prepared = True

    def get_features_absolute(self, n: int) -> Iterable[numpy.ndarray]:
        self._prepare()

        top_n_words = numpy.array([w for (w, f) in self._avg_freq_dist.most_common(n)])
        num_top_words = len(top_n_words)
        for c in self._chunks:
            vec = numpy.zeros(2 * n)

            self.__freq_a = FreqDist(self._tokenize(c[0]))

            for i in range(0, n):
                if i >= num_top_words:
                    break
                vec[i] = self.__freq_a[top_n_words[i]]

            self.__freq_b = FreqDist(self._tokenize(c[1]))

            for i in range(n, 2 * n):
                if i >= num_top_words + n:
                    break
                vec[i] = self.__freq_b[top_n_words[i - n]]

            yield vec

    def get_features_relative(self, n: int) -> Iterable[numpy.ndarray]:
        features = self.get_features_absolute(n)
        for vec in features:
            n_a = self.__freq_a.N()
            for i in range(0, n):
                vec[i] /= n_a
            n_b = self.__freq_b.N()
            for i in range(n, 2 * n):
                vec[i] /= n_b
        
            yield vec

    @lru_cache(maxsize=200)
    def _tokenize(self, text) -> List[str]:
        return list(self._chunk_tokenizer.tokenize(text))


class AvgWordFreqFeatureSet(CachedAvgTokenCountFeatureSet):
    """
    Feature set using the average frequencies of the n most
    frequent words in both input chunk sets.
    """
    
    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None):
        super().__init__(pair, sampler, WordTokenizer())


class AvgCharNgramFreqFeatureSet(CachedAvgTokenCountFeatureSet):
    """
    Feature set using the average frequencies of the k most
    frequent character n-grams in both input chunk sets.

    Default n-gram order is 3.
    """

    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None):
        self.__tokenizer = CharNgramTokenizer(3)
        super().__init__(pair, sampler, self.__tokenizer)

    @property
    def order(self) -> int:
        """ Get n-gram order. """
        return self.__tokenizer.order

    @order.setter
    def order(self, ngram_order: int):
        """ Set n-gram order. """
        self.__tokenizer.order = ngram_order


class AvgDisjunctCharNgramFreqFeatureSet(CachedAvgTokenCountFeatureSet):
    """
    Feature set using the average frequencies of the k most
    frequent character n-grams in both input chunk sets.

    Default n-gram order is 3.
    """

    def __init__(self, pair: SamplePair = None, sampler: ChunkSampler = None):
        self.__tokenizer = DisjunctCharNgramTokenizer(3)
        super().__init__(pair, sampler, self.__tokenizer)

    @property
    def order(self) -> int:
        """ Get n-gram order. """
        return self.__tokenizer.order

    @order.setter
    def order(self, ngram_order: int):
        """ Set n-gram order. """
        self.__tokenizer.order = ngram_order
